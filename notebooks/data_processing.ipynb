{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Maestro dataset, tokenize, split, collate, and make a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the maestro and Lakh datasets\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def download_file(url, dest):\n",
    "    if not dest.exists():\n",
    "        response = requests.get(url, stream = True)\n",
    "        with open(dest, \"wb\")  as f:\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "    else:\n",
    "        print(f\"{dest} already exists. Skipping download.\")\n",
    "        \n",
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def extract_tar(tar_path, extract_to):\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(extract_to)\n",
    "\n",
    "maestro_url = \"https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\"\n",
    "maestro_zip_path = DATA_DIR / \"maestro-v.3.0.0.zip\"\n",
    "maestro_extract_path = DATA_DIR / \"maestro\"\n",
    "\n",
    "\n",
    "#Only download if not exist\n",
    "if not maestro_extract_path.exists():\n",
    "    download_file(maestro_url, maestro_zip_path)\n",
    "    extract_zip(maestro_zip_path, maestro_extract_path)\n",
    "    maestro_zip_path.unlink()  # Remove the zip file after extraction\n",
    "\n",
    "lmd_url = \"http://hog.ee.columbia.edu/craffel/lmd/lmd_full.tar.gz\"\n",
    "lmd_tar_path = DATA_DIR / \"lmd_full.tar.gz\"\n",
    "lmd_extract_path = DATA_DIR / \"lakh\"\n",
    "\n",
    "if not lmd_extract_path.exists():\n",
    "    download_file(lmd_url, lmd_tar_path)\n",
    "    extract_tar(lmd_tar_path, lmd_extract_path)\n",
    "    lmd_tar_path.unlink()  # Remove the tar file after extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikaelhokkanen/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "MAX_SEQ_LEN = 1024\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = 40000\n",
    "DATA_DIR = Path(\"data\")\n",
    "TOKENIZER_CONFIG = TokenizerConfig(\n",
    "    pitch_range=[21, 109],\n",
    "    beat_res={(0, 4): 16},\n",
    "    num_velocities=16,\n",
    "    use_chords=True,\n",
    "    use_tempos=True,\n",
    ")\n",
    "TOKENIZER_PATH = DATA_DIR / \"combined_tokenizer.json\"\n",
    "CHUNKS_DIR = DATA_DIR / \"combined_dataset_chunks\"\n",
    "# ===================================\n",
    "\n",
    "def get_maestro_midi_paths():\n",
    "    maestro_dir = (DATA_DIR / \"maestro\" / \"maestro-v3.0.0\").resolve()\n",
    "    metadata_path = maestro_dir / \"maestro-v3.0.0.csv\"\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    return [maestro_dir / x for x in metadata[\"midi_filename\"]]\n",
    "\n",
    "def get_lakh_midi_paths(limit=None):\n",
    "    lakh_root = (DATA_DIR / \"lakh\" / \"lmd_full\").resolve()\n",
    "    midi_paths = list(lakh_root.rglob(\"*.mid\"))\n",
    "    return midi_paths[:limit] if limit else midi_paths\n",
    "\n",
    "def prepare_and_tokenize_dataset():\n",
    "    if CHUNKS_DIR.exists():\n",
    "        print(\"Tokenized dataset chunks already exist. Skipping creation.\")\n",
    "        tokenizer = REMI(params=TOKENIZER_PATH)\n",
    "        return tokenizer\n",
    "\n",
    "    # Get MIDI file paths\n",
    "    maestro_paths = get_maestro_midi_paths()\n",
    "    lakh_paths = get_lakh_midi_paths(limit=2000)  # Optional: limit for dev\n",
    "\n",
    "    all_midi_paths = maestro_paths + lakh_paths\n",
    "\n",
    "    # Initialize and train tokenizer\n",
    "    tokenizer = REMI(tokenizer_config=TOKENIZER_CONFIG)\n",
    "    print(\"Training tokenizer on combined dataset...\")\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=all_midi_paths)\n",
    "    tokenizer.save(TOKENIZER_PATH)\n",
    "\n",
    "    # Tokenize and split\n",
    "    print(f\"Tokenizing and splitting into chunks...\")\n",
    "    split_files_for_training(\n",
    "        files_paths=all_midi_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=CHUNKS_DIR,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "    )\n",
    "    print(\"Tokenization complete.\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def load_dataset(tokenizer, max_seq_len):\n",
    "    # Load dataset\n",
    "    files = list(CHUNKS_DIR.glob(\"**/*.midi\"))\n",
    "    print(f\"Loading {len(files)} MIDI chunks...\")\n",
    "\n",
    "    dataset = DatasetMIDI(\n",
    "        files_paths=files,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset chunks already exist. Skipping creation.\n",
      "Loading 12061 MIDI chunks...\n",
      "Dataloaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "tokenizer = None\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = prepare_and_tokenize_dataset()\n",
    "dataset = load_dataset(tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "BOS_TOKEN_ID = tokenizer[\"BOS_None\"]\n",
    "EOS_TOKEN_ID = tokenizer[\"EOS_None\"]\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "\n",
    "collator = DataCollator(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    copy_inputs_as_labels=True,\n",
    "    shift_labels=False,\n",
    "    pad_on_left=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collator, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "\n",
    "print(\"Dataloaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XL-transformer decoder time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransfoXLLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntraining_arg = TrainingArguments(\\n    output_dir = OUTPUT_DIR,\\n    per_device_train_batch_size=BATCH_SIZE,\\n    per_device_eval_batch_size=BATCH_SIZE,\\n    eval_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    num_train_epochs=NUM_EPOCHS,\\n    logging_dir=\"./logs\",\\n    logging_steps=50,\\n    save_total_limit=2,\\n    learning_rate=5e-5,\\n    weight_decay=0.01,\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transformer XL model\n",
    "from transformers import TransfoXLConfig, TransfoXLLMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = REMI(params=TOKENIZER_PATH)\n",
    "\n",
    "OUTPUT_DIR = Path(\"checkpoint/transformer_xl\")\n",
    "\n",
    "config = TransfoXLConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    d_embed = 64,\n",
    "    d_model = 64,\n",
    "    n_layer=2,\n",
    "    n_head=2,\n",
    "    mem_len=256,\n",
    "    clamp_len=0,\n",
    "    cutoffs = [],\n",
    "    adaptive = False,\n",
    "    eos_token_id=tokenizer[\"EOS_None\"]\n",
    ")\n",
    "\n",
    "model = TransfoXLLMHeadModel(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.596830368041992\n",
      "iteration\n",
      "21.19341278076172\n",
      "iteration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py:1110\u001b[0m, in \u001b[0;36mTransfoXLLMHeadModel.forward\u001b[0;34m(self, input_ids, mems, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m miss_valid_label:\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;66;03m# Sets an <EOS> token, just to prevent loss from being NaN\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m         labels[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m-> 1110\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_hid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m softmax_output\u001b[38;5;241m.\u001b[39mview(bsz, tgt_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/midi-transformer/lib/python3.10/site-packages/transformers/models/deprecated/transfo_xl/modeling_transfo_xl_utilities.py:114\u001b[0m, in \u001b[0;36mProjectedAdaptiveLogSoftmax.forward\u001b[0;34m(self, hidden, labels, keep_order)\u001b[0m\n\u001b[1;32m    111\u001b[0m     mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    112\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(labels, dtype\u001b[38;5;241m=\u001b[39mhidden\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mhidden\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    113\u001b[0m     out[mask] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(logit, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "OUTPUT_DIR = Path(\"checkpoint/transformer_xl\")\n",
    "LOG_DIR  = Path(\"logs\")\n",
    "BATCH_SIZE=1\n",
    "NUM_EPOCHS = 1\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(total_loss)\n",
    "\n",
    "        #logits of size [batch_size*seq_len, vocab_size]\n",
    "        #labels of size [batch_size*seq_len]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"iteration\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    #Eval\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "    \n",
    "    avg_eval_loss = total_eval_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    model.save_pretrained(OUTPUT_DIR / f\"epoch_{epoch + 1}\")\n",
    "\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midi-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
